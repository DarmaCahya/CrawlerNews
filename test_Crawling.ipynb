{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5cl+CV8Ll98Wo3uvfVbUF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarmaCahya/CrawlerNews/blob/main/test_Crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUW-C6KugHFf",
        "outputId": "1e4ec7da-390e-4f87-93e2-5d429412265b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling data secara umum tanpa menggunakan topik"
      ],
      "metadata": {
        "id": "6Xo2Xv6Lat35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open('news_websites.json') as json_file:\n",
        "    websites = json.load(json_file)['websites']\n",
        "\n",
        "def get_soup(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def parse_article(link, website):\n",
        "    try:\n",
        "        soup = get_soup(link)\n",
        "        content_div = soup.find(class_=website['content_class'])\n",
        "        content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p')) if content_div else 'No content found'\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing article {link}: {e}\")\n",
        "        return 'No content found'\n",
        "\n",
        "def parse_page(url, website):\n",
        "    soup = get_soup(url)\n",
        "    news_data = []\n",
        "    articles = soup.find_all(website['article_tag'], class_=website['article_class'])\n",
        "    print(f\"Found {len(articles)} articles on page: {url}\")\n",
        "\n",
        "    for article in articles:\n",
        "        title_tag = article.find(class_=website.get('title_class', None))\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text(strip=True)\n",
        "        else:\n",
        "            title_tag = article.find('a')\n",
        "            title = title_tag['title'] if 'title' in title_tag.attrs else title_tag.get_text(strip=True)\n",
        "\n",
        "        link_tag = article.find('a')\n",
        "        link = link_tag['href'] if link_tag else 'No link found'\n",
        "\n",
        "        date_tag = article.find(class_=website['date_class'])\n",
        "        date = date_tag.get_text(strip=True) if date_tag else 'No date found'\n",
        "\n",
        "        content = parse_article(link, website) if link != 'No link found' else 'No content found'\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'date': date,\n",
        "            'content': content,\n",
        "            'is_fake': 0,\n",
        "            'media_bias': website['platform']\n",
        "        })\n",
        "        print(f\"Appended article: {title}\")\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def get_all_articles(base_url, website, max_pages=2):\n",
        "    articles = []\n",
        "    next_page = base_url\n",
        "    current_page = 1\n",
        "\n",
        "    while next_page and current_page <= max_pages:\n",
        "        print(f\"Crawling page {current_page}\")\n",
        "        articles.extend(parse_page(next_page, website))\n",
        "        soup = get_soup(next_page)\n",
        "        if website['name'] == 'Antara':\n",
        "            next_page = f\"{base_url}/{current_page + 1}\"\n",
        "        elif website['name'] == 'Suara':\n",
        "            next_page = f\"{base_url}?page={current_page + 1}\"\n",
        "        elif website['name'] == 'Detik':\n",
        "            next_page = f\"{base_url}/{current_page + 1}\"\n",
        "        else:\n",
        "            next_button = soup.find(class_=website['next_page'])\n",
        "            next_page = next_button[\"href\"] if next_button else None\n",
        "        current_page += 1\n",
        "        time.sleep(2)\n",
        "    return articles\n",
        "\n",
        "def main():\n",
        "    all_news = []\n",
        "    for website in websites:\n",
        "        try:\n",
        "            base_url = website['url']\n",
        "            scraped_news = get_all_articles(base_url, website)\n",
        "            print(f\"Scraped {len(scraped_news)} articles from {website['name']}\")\n",
        "            all_news.extend(scraped_news)\n",
        "            time.sleep(2)  # Respectful delay to avoid overwhelming the server\n",
        "        except requests.HTTPError as e:\n",
        "            print(f\"Failed to scrape {website['name']}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_news)\n",
        "    print(f\"Total articles collected: {len(all_news)}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('scraped_news.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "B02dwkxdgK_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b35a43dc-f0b0-4dba-ccad-0b674c4a2cc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling page 1\n",
            "Found 10 articles on page: https://www.cnnindonesia.com/politik/indeks/4\n",
            "Appended article: Paripurna DPR Resmi Sahkan RUU Sumber Daya Alam Hayati jadi UU\n",
            "Appended article: Airlangga Ungkap Rencana Kaesang Akan Bertamu ke Golkar Kamis Esok\n",
            "Appended article: DPR Berencana Ubah Wantimpres Kembali Jadi DPA\n",
            "Appended article: Guyon Kaesang: Pak Presiden PKS Harusnya Jadi Calon Gubernur Jakarta\n",
            "Appended article: PDIP Sindir IKN Molor: Sebelumnya Bilang Sangat Siap, Ternyata Belum\n",
            "Appended article: DPR Segera Jadwalkan Rapat Bahas Putusan MA soal Syarat Usia Cagub\n",
            "Appended article: Menag Yaqut Buka Suara soal DPR Bentuk Pansus Angket Haji\n",
            "Appended article: PBNU Sebut Izin Kelola Tambang Belum Keluar: Masih Proses\n",
            "Appended article: Pansus Angket Pengawasan Haji 2024 Diteken 35 Anggota DPR\n",
            "Appended article: DPR Resmi Bentuk Pansus Angket Haji Beranggotakan 30 Orang\n",
            "Crawling page 2\n",
            "Found 10 articles on page: https://www.cnnindonesia.com/politik/indeks/4/2\n",
            "Appended article: Jokowi Belum Bisa Pastikan Berkantor di IKN Nusantara Juli Ini\n",
            "Appended article: VIDEO: Jokowi soal Kaesang Maju Pilkada: Orang Tua Hanya Mendoakan\n",
            "Appended article: Kenapa Banyak Parpol Dukung Bobby Nasution Ketimbang Edy Rahmayadi?\n",
            "Appended article: PKS Usul Seluruh Perwakilan Fraksi Jadi Pimpinan DPR Seperti di MPR\n",
            "Appended article: KPU Respons Mahfud MD soal Komisioner Pakai 3 Mobil Dinas dan Sewa Jet\n",
            "Appended article: Jokowi Belum Teken Keppres Pemecatan Hasyim Asyari dari Ketua KPU\n",
            "Appended article: Dasco Sebut Belum Bahas Kaesang dan Ahmad Luthfi di Jateng Bareng KIM\n",
            "Appended article: Jokowi Buka Suara usai KPU Dianggap Tak Layak Selenggarakan Pilkada\n",
            "Appended article: Belum Juga Terbitkan Keppres IKN, Jokowi Mengaku Tak Mau Paksakan\n",
            "Appended article: Jokowi Kirim Bantuan Rp35,5 Miliar untuk Papua Nugini dan Afghanistan\n",
            "Scraped 20 articles from CNN Indonesia\n",
            "Crawling page 1\n",
            "Found 20 articles on page: https://news.detik.com/berita/indeks\n",
            "Appended article: KemenPPPA Dorong Kasus Ketua OSIS di Klaten Tewas Kesetrum Diusut Tuntas\n",
            "Appended article: Buntut Kasus Hasyim, Anggota DPR Minta Penjaringan Komisioner KPU Diperketat\n",
            "Appended article: KPK Dalami Dugaan Pegawainya Main Judi Online Saat Jam Kerja\n",
            "Appended article: KPK Tegaskan AKBP Rossa Kantongi Surat Saat Geledah Rumah Advokat PDIP\n",
            "Appended article: Temuan Ombudsman di PPDB Banten: Mark Up Nilai hingga Numpang KK\n",
            "Appended article: 26 RUU Kabupaten dan Kota Disetujui DPR Jadi Undang-Undang di Paripurna\n",
            "Appended article: Polisi Dirikan Pos Pantau Cegah Tawuran Kembali Pecah di Bassura Jaktim\n",
            "Appended article: Nilai Judi Online Pegawai KPK Terbesar Rp 74 Juta Hasil 300 Kali Main\n",
            "Appended article: Jambret Dibogem Warga Usai Curi HP Bocah di Depok, 1 Pelaku Melarikan Diri\n",
            "Appended article: Pimpinan KPK soal Pegawai Terlibat Judi Online: Mungkin Lagi Iseng-Nganggur\n",
            "Appended article: Harun Masiku Ada di Jakarta? KPK: Saya Nggak Tahu Ngumpetnya di Mana\n",
            "Appended article: Gempa M 4,3 Guncang Halmahera Timur Malut\n",
            "Appended article: Kala Tower di Atas Masjid Jakut Bikin Waswas Warga\n",
            "Appended article: Kapolresta Bogor Terima Penghargaan KememPPPA Tangani Anak Terlibat Tawuran\n",
            "Appended article: Pimpinan KPK soal AKBP Rossa Kembali Dilaporkan PDIP ke Dewas: Silakan Saja\n",
            "Appended article: Tawuran di Bassura Jaktim Reda, Lalin Kembali Normal\n",
            "Appended article: Polisi: Kasus Pria Disekap 3 Bulan di Kafe Jaktim Dipicu Masalah Utang\n",
            "Appended article: Ketum PP Muhammadiyah Serukan Hijrah ke Kalender Islam Global Tunggal\n",
            "Appended article: Legislator PDIP Minta KPK Tak Ragu Pecat Pegawai yang Terlibat Judi Online\n",
            "Appended article: Bareskrim: Jaringan Fredy Pratama Ubah Pola Pengedaran Narkoba di RI\n",
            "Crawling page 2\n",
            "Found 20 articles on page: https://news.detik.com/berita/indeks/2\n",
            "Appended article: Pungli Belasan Miliar di Raja Ampat Papua Dibongkar KPK\n",
            "Appended article: KPK Tahan 3 Tersangka Dugaan Korupsi PLTU di Sumsel\n",
            "Appended article: DPRD Kota Surabaya Dorong Pemkot Lebih Hati-hati Terkait Pemblokiran KK\n",
            "Appended article: Pelaku Curi Data Pelamar Kerja di Jaktim Janjikan Korban Bekerja Jadi Admin\n",
            "Appended article: Kebakaran Rumah di Menteng, Damkar Terkendala Akses-Terpaksa Dobrak Pintu\n",
            "Appended article: Kebakaran Rumah di Menteng Jakpus Diduga karena Korsleting Listrik\n",
            "Appended article: Menteri PUPR Targetkan Air Bersih Masuk ke IKN Mulai 15 Juli\n",
            "Appended article: UU KIA Atur Cuti Lahiran Bisa 6 Bulan, Muhadjir: Siapkan Generasi Emas\n",
            "Appended article: Total Transaksi Judi Online Pegawai KPK Capai Rp 111 Juta\n",
            "Appended article: KPK soal 17 Orang Terlibat Judi Online: 9 Sudah Bukan Pegawai\n",
            "Appended article: Apa Saja Dokumen Kependudukan dalam Format Digital? Cek Infonya\n",
            "Appended article: Aktivis Muda Ajak Masyarakat Terus Ikut Lawan Judi Online\n",
            "Appended article: Korban Penyekapan di Jaktim Trauma, Teriak Lihat Mobil Mirip Punya Pelaku\n",
            "Appended article: Fadel Muhammad Ungkap Peran DPD dalam Penerapan Collaborative Governance\n",
            "Appended article: Satpol PP Segel Dua Warung Kelontong Penjual Miras Ilegal di Kota Bogor\n",
            "Appended article: Wanita Nekat Lompat dari Bus di Tol Cipularang, Diduga Depresi\n",
            "Appended article: Pria Disekap 3 Bulan di Jaktim: Kepala Dihantam Gas 3 Kg-Dipaksa Makan Batu\n",
            "Appended article: Tawuran Warga Kembali Pecah di Bassura Jaktim, Lalin Sempat Macet\n",
            "Appended article: Prof BUS Minta Maaf ke Rektor Unair Usai Kembali Jabat Dekan FK\n",
            "Appended article: Syarat Daftar Nikah ke KUA, Calon Pasutri Wajib Tahu!\n",
            "Scraped 40 articles from Detik\n",
            "Crawling page 1\n",
            "Found 15 articles on page: https://www.kompas.com/tag/politik\n",
            "Appended article: Dukungan PKS untuk Anies pada Pilkada Jakarta Dianggap Rezeki Politik, Bukan Hambatan\n",
            "Appended article: Pesan Gibran soal Pilkada 2024: Kaesang Jangan di Jakarta Lah\n",
            "Appended article: Pilih Dunia Politik, Jeje Govinda Tolak Tawaran Raffi Ahmad Jadi Komisaris RANS\n",
            "Appended article: PKS Usul Semua Parpol Dapat Kursi Pimpinan DPR, Bamsoet: Saya Menyambut Baik\n",
            "Appended article: Jawab Megawati, KPK Klaim Tak Targetkan Afiliasi Politik Tersangka Korupsi\n",
            "Appended article: Gus Yusuf Tak Masuk 5 Besar Survei Indikator Politik, PKB: 3 Parpol Sudah Dukung\n",
            "Appended article: Megawati: Keren Lho, Saya Ini! #Shorts\n",
            "Appended article: Megawati Tantang Penyidik Rossa Menghadap, KPK Diminta Jangan Gentar\n",
            "Appended article: Respons Kaesang Digadang Jadi Pemimpin di Jateng\n",
            "Appended article: Survei Indikator Politik: Kaesang Tertinggi di Jateng, Disusul Ahmad Luthfi\n",
            "Appended article: Survei Indikator Politik: Ahmad Luthfi Teratas dalam 'Top of Mind’ Pilkada Jateng\n",
            "Appended article: Survei Indikator Politik: Kaesang Raih Elektabilitas Tertinggi di Jateng, Disusul Ahmad Luthfi\n",
            "Appended article: Mahfud Ajak Masyarakat Dukung Pemerintahan ke Depan Agar Indonesia Emas 2045 Tercapai\n",
            "Appended article: Mahfud: Keadilan dan Kemakmuran Sebagai Syarat Indonesia Emas Belum Terwujud\n",
            "Appended article: Momen Megawati 2 Kali Sebut Nama Jokowi Usai \"Huru-hara\" Pilpres\n",
            "Crawling page 2\n",
            "Found 15 articles on page: https://www.kompas.com/tag/politik?page=2\n",
            "Appended article: Alasan KPU Tak Minta Maaf Usai Ketua KPU Hasyim Langgar Etik\n",
            "Appended article: Heru Budi Bisa Saja Ikut Pilkada, tapi Dianggap Sulit Diusung Partai Politik\n",
            "Appended article: Wapres: Isu Palestina Bukan soal Agama, tapi Politik dan Kemanusiaan\n",
            "Appended article: Bobby Nasution Ungkap Peran Jokowi dalam Pencalonannya di Pilkada Sumut, Apa Itu?\n",
            "Appended article: Survei Indikator Politik: Kans Dedi Mulyadi jika Ridwan Kamil Ikut Pilkada Jakarta\n",
            "Appended article: Survei Indikator Politik: Elektabilitas Ridwan Kamil, Dedi Mulyadi, dan Komeng Tertinggi di Jabar\n",
            "Appended article: Pilih Usung Bobby Ketimbang Edy Rahmayadi di Pilkada Sumut, PKB: Dia Akan Menang\n",
            "Appended article: PKB Usulkan Nagita Slavina Jadi Cawagub Bobby Nasution di Pilkada Sumut 2024\n",
            "Appended article: Survei Indikator: Elektabilitas Ridwan Kamil Teratas di Jawa Barat, Dedi Mulyadi Nomor Dua\n",
            "Appended article: [FULL] PKB Tetapkan Bobby Nasution sebagai Calon Gubernur Sumatera Utara\n",
            "Appended article: Puan Tersenyum Tanggapi Wacana Duet Anies-Andika di Jakarta\n",
            "Appended article: Survei Indikator Politik Indonesia: Persaingan Pilkada Jabar Hanya Ridwan Kamil dan Dedi Mulyadi\n",
            "Appended article: Respons Puan soal Pemecatan Ketua KPU karena Asusila\n",
            "Appended article: Akankah Ancaman Perang Hizbullah-Israel bisa Picu Konflik Sipil di Lebanon?\n",
            "Appended article: PKS Pertimbangkan Narji untuk Diusung Jadi Cawalkot Tangsel Pilkada 2024\n",
            "Scraped 30 articles from Kompas\n",
            "Crawling page 1\n",
            "Found 19 articles on page: https://www.suara.com/tag/politik\n",
            "Appended article: Potret Kaesang Pakai Kimono Batik saat Temui Presiden PKS\n",
            "Appended article: Silaturahmi Kebangsaan, Bambang Soesatyo Temui Presiden PKS\n",
            "Appended article: Popularitas Kaesang Disebut Paling Tinggi Di Jateng, Jokowi Jadi Faktor Utama\n",
            "Appended article: Survei Indikator Politik: Kaesang Ungguli Irjen Luthfi Di Pilkada Jateng\n",
            "Appended article: Survei Pilkada Jabar 2024: Ridwan Kamil Masih Favorit, Tapi Komeng Ikut Ngekor\n",
            "Appended article: Siapa Kader Gerindra yang Bertarung di Pilbup Bogor? Sosok Ini Disebut Punya Nilai Tinggi\n",
            "Appended article: Ketua ICMI: Sistem Politik Kita Makin Tidak Inklusif\n",
            "Appended article: Pengamat Beberkan Kemungkinan Golkar Tak Dapat Koalisi di Pilgub Banten\n",
            "Appended article: Bukan Pengusaha! Terungkap Pekerjaan yang Diincar Azriel Hermansyah Setelah Nikahi Sarah Menzel\n",
            "Appended article: Bila Ridwan Kamil ke Jakarta, Dedi Mulyadi Bakal Dapat Limpahan Suara di Pilgub Jabar\n",
            "Appended article: Ada Dedi Mulyadi di Jabar, Golkar Masih Tarik Ulur Bawa Ridwan Kamil ke Jakarta?\n",
            "Appended article: Momen Ridwan Kamil Skakmat Panji Pragiwaksono Saat Singgung Soal Politik Dinasti\n",
            "Appended article: Sosok Jusuf Wanandi, Petinggi Total Politik Sempat Sebut Anies Tak Bisa Kerja\n",
            "Appended article: Soal Aturan Usia Muda Calon Kepala Daerah, Mardani Ali Sera: Kasihan Publik\n",
            "Appended article: Mengulik Sepak Terjang Pujiyono Suwandi, Ketua Komisi Kejaksaan yang Dirumorkan Maju Pilkada Boyolali\n",
            "Appended article: Tobat Politik! Angelina Sondakh Ungkap Alasan Tak Mau Lagi Jadi Pejabat\n",
            "Appended article: Apa Itu Politik Uang? Ada 4 Kategori Dalam Pilkada\n",
            "Appended article: Pilkada Serentak 27 November 2024, Masyarakat Diminta Tidak Memilih Karena Uang\n",
            "Appended article: Prediksi Pilkada 2024: Kaum Nepotisme dan Dinasti Politik Berpesta, Politik Uang dan Bansos Bakal Merajalela\n",
            "Crawling page 2\n",
            "Found 19 articles on page: https://www.suara.com/tag/politik?page=2\n",
            "Appended article: 6 Parpol Besar di Solo Bertemu dan Bakal Bentuk Koalisi Besar, Partai Golkar: Baru Wacana, Belum Ada Kesepakatan\n",
            "Appended article: Bambang Soesatyo\n",
            "Appended article: Asal Usul Gelar Haji di Indonesia: Dijadikan Modal Politik Raffi Ahmad, Padahal Dulunya Akal-akalan Penjajah\n",
            "Appended article: Ini Kata Istana soal Pergantian Pj Gubernur di Sejumlah Provinsi\n",
            "Appended article: Politik Uang di Pilkada Sulawesi Selatan Jadi Masalah Utama\n",
            "Appended article: Ulang Tahun ke-63 Hari Ini, Jokowi Dapat Ucapan dari Sejumlah Tokoh Politik, Ini Isinya!\n",
            "Appended article: Kasih Ucapan Ulang Tahun, Anies Baswedan Doakan Jokowi Dapat Petunjuk saat Jalani Amanah\n",
            "Appended article: Disiapkan Gerindra untuk Lawan Benyamin Davnie, Marshel Widianto Malah Disebut Badut Politik\n",
            "Appended article: Golkar Usung Bobby Nasution Ketimbang Ijeck di Pilgub Sumut, Pengamat Sindir Dahsyatnya Dinasti Politik\n",
            "Appended article: Camat dan Lurah di Cilegon Diminta Berkompetisi Paparkan Janji Politik dan Program Prioritas\n",
            "Appended article: Blak-blakan Anies Baswedan Nilai Jokowi Segini soal Berpolitik Bersih di Indonesia: Belum Jadi Prioritas\n",
            "Appended article: Bawaslu Lampung Siap Perangi Politik Uang di Pilkada 2024, Bansos Jadi Sorotan!\n",
            "Appended article: Bikin Giveaway Tasyakuran Kehamilan, Erina Gudono Ramai Disindir: Nanti Anaknya Dapat Jabatan Apa?\n",
            "Appended article: Beredar Surat Dukungan Caleg DPR RI, Kesepuhan: Jangan Bawa Orang Baduy ke Urusan Politik\n",
            "Appended article: Sah! Ini 40 Nama Anggota DPRD Kulon Progo Periode 2024-2029\n",
            "Appended article: Pecah Kongsi, Dua Kader PPP Saling Demo di Depan Kantor DPP\n",
            "Appended article: Riwayat Pendidikan Ngabalin, Sebut Tak Ada Masalah Keluarga Jokowi Jadi Petinggi BUMN\n",
            "Appended article: PKB yang Memulai, Anies Senang Dapat Amanah Jadi Calon Gubernur Jakarta Lagi\n",
            "Appended article: Dua Politisi Golkar Jaro Ade dan Sulhajji Jompa Punya Peluang Kuat Dapatkan Tiket dari PKB, Kok Bisa?\n",
            "Scraped 38 articles from Suara\n",
            "Crawling page 1\n",
            "Found 15 articles on page: https://www.antaranews.com/politik\n",
            "Appended article: Menko Polhukam beri arahan untuk sukses Pilkada 2024 wilayah Sumatera\n",
            "Appended article: TNI AL dan USMC latihan pakai drone intai dan komunikasi taktis\n",
            "Appended article: Wapres pertimbangkan tambah anggota KPU imbas dipecatnya Hasyim\n",
            "Appended article: Polres Rote-NTT kumpulkan pakaian bekas untuk 44 WNA yang terdampar\n",
            "Appended article: Kopaska dan US Navy SEALs latihan peperangan laut khusus di Jatim\n",
            "Appended article: Aura nasionalisme di balik revitalisasi Kota Lama Surabaya\n",
            "Appended article: Pangkoarmada II tinjau latihan TNI AL di Latma Rimpac Hawaii\n",
            "Appended article: KPU Pariaman tingkatkan sosialisasi PSU DPD RI\n",
            "Appended article: Tito kembali tegaskan ASN wajib jaga netralitas di Pilkada 2024\n",
            "Appended article: Kompolnas kunjungi Polres Jembrana pantau pengamanan pilkada\n",
            "Appended article: Mendagri tekankan pentingnya tingkatkan partisipasi pemilih di pilkada\n",
            "Appended article: Pemerintah konsisten kawal percepatan pembangunan di Papua Pegunungan\n",
            "Appended article: Anggota DPR minta pemerintah tingkatkan tata keuangan usai raih WTP\n",
            "Appended article: Akhmad Munir: Mendiang Kamsul Hasan Sosok aktif advokasi hukum pers\n",
            "Appended article: Dewan Adat Papua Teluk Wondama serahkan berkas calon anggota DPRK\n",
            "Crawling page 2\n",
            "Found 15 articles on page: https://www.antaranews.com/politik/2\n",
            "Appended article: Kepala BSKDN minta pemkab kembangkan inovasi berbasis potensi desa\n",
            "Appended article: Wapres minta KPU perkuat dan benahi instansi untuk kelancaran Pilkada\n",
            "Appended article: Wakil Ketua DPR RI sebut ucapan Mahfud sebagai masukan penting\n",
            "Appended article: Danrem 172/PWY: Yonif 614/RJP gabung satgas pamtas kewilayahan\n",
            "Appended article: Sosiolog UNS soroti isu majunya Gusti Bhre pada Pilkada Surakarta\n",
            "Appended article: Wapres minta peristiwa salah tangkap seperti Pegi tidak terulang\n",
            "Appended article: Pemerintah Madiun percepat perekaman KTP-el pemilih pemula\n",
            "Appended article: Golkar: KPU lanjut selenggarakan pilkada selama tak ada pelanggaran\n",
            "Appended article: Petugas Pantarlih Cimahi termukan satu rumah dihuni oleh 46 jiwa\n",
            "Appended article: Mendagri imbau pemda se-Sumatera segera realisasikan anggaran pilkada\n",
            "Appended article: Airlangga sebut penentuan pencalonan Kang Emil menunggu waktu\n",
            "Appended article: Bawaslu ke KPU usai pemberhentian Hasyim: Badai pasti berlalu\n",
            "Appended article: Sentra Gakumdu antisipasi pelanggaran pemilu empat titik di Sumatera\n",
            "Appended article: Asops Kapolri: Sinergisitas kepala daerah amankan Pilkada 2024\n",
            "Appended article: Airlangga ungkap Kaesang akan sambangi DPP Golkar Kamis\n",
            "Scraped 30 articles from Antara\n",
            "Crawling page 1\n",
            "Failed to scrape Tribunnews: 403 Client Error: Forbidden for url: https://www.tribunnews.com/nasional/politik\n",
            "Total articles collected: 158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling data menggunakan inputan dari user atau berdasarkan topik"
      ],
      "metadata": {
        "id": "_GDmxLKCbVL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "}\n",
        "\n",
        "# Load JSON file\n",
        "with open('news_websites.json') as json_file:\n",
        "    websites = json.load(json_file)['websites']\n",
        "\n",
        "def get_soup(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def parse_article(link, website):\n",
        "    try:\n",
        "        soup = get_soup(link)\n",
        "        content_div = soup.find(class_=website['content_class'])\n",
        "        content = ' '.join(p.get_text(strip=True) for p in content_div.find_all('p')) if content_div else 'No content found'\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing article {link}: {e}\")\n",
        "        return 'No content found'\n",
        "\n",
        "def parse_page(url, website):\n",
        "    soup = get_soup(url)\n",
        "    news_data = []\n",
        "    articles = soup.find_all(website['article_tag'], class_=website['article_class'])\n",
        "    print(f\"Found {len(articles)} articles on page: {url}\")\n",
        "\n",
        "    for article in articles:\n",
        "        title_tag = article.find(class_=website.get('title_class', None))\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text(strip=True)\n",
        "        else:\n",
        "            title_tag = article.find('a')\n",
        "            if title_tag:\n",
        "                title = title_tag['title'] if 'title' in title_tag.attrs else title_tag.get_text(strip=True)\n",
        "            else:\n",
        "                title = 'No title found'\n",
        "        link_tag = article.find('a')\n",
        "        link = link_tag['href'] if link_tag else 'No link found'\n",
        "\n",
        "        date_tag = article.find(class_=website['date_class'])\n",
        "        date = date_tag.get_text(strip=True) if date_tag else 'No date found'\n",
        "\n",
        "        content = parse_article(link, website) if link != 'No link found' else 'No content found'\n",
        "\n",
        "        news_data.append({\n",
        "            'title': title,\n",
        "            'link': link,\n",
        "            'date': date,\n",
        "            'content': content,\n",
        "            'is_fake': 0,\n",
        "            'media_bias': website['platform']\n",
        "        })\n",
        "        print(f\"Appended article: {title}\")\n",
        "\n",
        "    return news_data\n",
        "\n",
        "def get_all_articles(base_url, website, max_pages=2):\n",
        "    articles = []\n",
        "    next_page = base_url\n",
        "    current_page = 1\n",
        "\n",
        "    while next_page and current_page <= max_pages:\n",
        "        print(f\"Crawling page {current_page}\")\n",
        "        articles.extend(parse_page(next_page, website))\n",
        "        soup = get_soup(next_page)\n",
        "        if website['name'] == 'Antara':\n",
        "            next_page = f\"{base_url}&page={current_page + 1}\"\n",
        "        elif website['name'] == 'Suara':\n",
        "            next_page = f\"{base_url}?page={current_page + 1}\"\n",
        "        elif website['name'] == 'Detik':\n",
        "            next_page = f\"{base_url}&page={current_page + 1}\"\n",
        "        else:\n",
        "            next_button = soup.find(class_=website['next_page'])\n",
        "            next_page = next_button[\"href\"] if next_button else None\n",
        "        current_page += 1\n",
        "        time.sleep(2)\n",
        "    return articles\n",
        "\n",
        "def main():\n",
        "    all_news = []\n",
        "    topik = input(\"Masukkan Topik: \")\n",
        "    for website in websites:\n",
        "        try:\n",
        "            base_url = website['url'] + topik\n",
        "            scraped_news = get_all_articles(base_url, website)\n",
        "            print(f\"Scraped {len(scraped_news)} articles from {website['name']}\")\n",
        "            all_news.extend(scraped_news)\n",
        "            time.sleep(2)  # Respectful delay to avoid overwhelming the server\n",
        "        except requests.HTTPError as e:\n",
        "            print(f\"Failed to scrape {website['name']}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(all_news)\n",
        "    print(f\"Total articles collected: {len(all_news)}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('scraped_news.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "Y25gLE_sbaM3",
        "outputId": "25766ce7-4466-4507-e826-07226521e5ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukkan Topik: jakarta\n",
            "Crawling page 1\n",
            "Found 0 articles on page: https://www.suara.com/search?q=jakarta\n",
            "Crawling page 2\n",
            "Found 0 articles on page: https://www.suara.com/search?q=jakarta?page=2\n",
            "Scraped 0 articles from Suara\n",
            "Crawling page 1\n",
            "Found 15 articles on page: https://www.antaranews.com/search?q=jakarta\n",
            "Appended article: Estimasi kebutuhan baja 331 ribu ton dalam pembangunan IKN 2023-2024\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-de21887fe2f6>\u001b[0m in \u001b[0;36m<cell line: 108>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-de21887fe2f6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mbase_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtopik\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mscraped_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scraped {len(scraped_news)} articles from {website['name']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mall_news\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraped_news\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-de21887fe2f6>\u001b[0m in \u001b[0;36mget_all_articles\u001b[0;34m(base_url, website, max_pages)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mnext_page\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent_page\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_pages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Crawling page {current_page}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0marticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Antara'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-de21887fe2f6>\u001b[0m in \u001b[0;36mparse_page\u001b[0;34m(url, website)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_tag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate_tag\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No date found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'No link found'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No content found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         news_data.append({\n",
            "\u001b[0;32m<ipython-input-9-de21887fe2f6>\u001b[0m in \u001b[0;36mparse_article\u001b[0;34m(link, website)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwebsite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mcontent_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwebsite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_div\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent_div\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No content found'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-de21887fe2f6>\u001b[0m in \u001b[0;36mget_soup\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    792\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mtls_in_tls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}